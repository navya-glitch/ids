{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b54f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            index\n",
      "count  119.000000\n",
      "mean    14.075630\n",
      "std      8.135671\n",
      "min      0.000000\n",
      "25%     10.000000\n",
      "50%     14.000000\n",
      "75%     16.000000\n",
      "max     28.000000\n",
      "[[21  0]\n",
      " [ 3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        21\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.88        24\n",
      "   macro avg       0.44      0.50      0.47        24\n",
      "weighted avg       0.77      0.88      0.82        24\n",
      "\n",
      "Accuracy: 87.5\n",
      "Accuracy: 0.875\n",
      "\n",
      "\n",
      "\tResult Generation\n",
      "\t------------------\n",
      "\n",
      "\tTrue positive =  100.0 %\n",
      "\n",
      "\tFalse positive =  0.0 %\n",
      "\n",
      "\tFalse negative =  100.0 %\n",
      "\n",
      "\tTrue negative =  0.0 %\n",
      "\n",
      "\tAccuracy =  50.0 %\n",
      "\n",
      "\tError Rate =  50.0 %\n",
      "\n",
      "\tPrecision =  100.0 %\n",
      "\n",
      "\tRecall =  50.0 %\n",
      "\n",
      "\tF1-Score =  66.66666666666667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#import dataset\n",
    "Dataset=pd.read_csv('files.csv')\n",
    "print(Dataset.describe())\n",
    "\n",
    "x=Dataset.iloc[:,:-1].values\n",
    "x1=pd.DataFrame(x)\n",
    "y=Dataset.iloc[:,10].values\n",
    "y1=pd.DataFrame(y)\n",
    "\n",
    "for i in range(119):\n",
    "    if y[i]=='anom':\n",
    "        y[i]=0\n",
    "    else:\n",
    "        y[i]=1\n",
    "type(y)\n",
    "type(x)\n",
    "y=y.astype('int')\n",
    "\n",
    "#Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "labelencoder_x=LabelEncoder()\n",
    "for i in range(10) :\n",
    "    x[:,i]=labelencoder_x.fit_transform(x[:,i])\n",
    "    Y=pd.DataFrame(x[:,i])\n",
    "for i in range(10) :\n",
    "    #onehotencoder=OneHotEncoder(categorical_features=[i])\n",
    "    #x=onehotencoder.fit_transform(x).toarray()\n",
    "    onehotencoder = OneHotEncoder()\n",
    "    X = onehotencoder.fit_transform(x).toarray()\n",
    "    X = X[:, 1:]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#write in file\n",
    "np.savetxt('encode_valuse.txt',x)\n",
    "\n",
    "#Missing Data Removal\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values='NaN', strategy='most_frequent')\n",
    "imputer = imputer.fit(x[:,:])\n",
    "x[:,:]=imputer.transform(x[:,:])\n",
    "Missing_Data_Removed=imputer.transform(x[:,:])\n",
    "\n",
    "#write in file\n",
    "np.savetxt('Missing_values.txt',Missing_Data_Removed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0) \n",
    " \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "#SVM Apply\n",
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "dt = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  \n",
    "#performance anaylsis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "from sklearn import metrics \n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred)*100)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "Total_TP_FP=cm[0][0]+cm[0][1]\n",
    "Total_FN_TN=cm[1][0]+cm[1][1]\n",
    "\n",
    "#True Positive Calculation\n",
    "TP1=(((cm[0][0])/Total_TP_FP)*100)\n",
    "\n",
    "#False Positive Calculation\n",
    "FP1= (((cm[0][1])/Total_TP_FP)*100)\n",
    "\n",
    "#False Negative Calculation\n",
    "FN1=(((cm[1][0])/Total_FN_TN)*100)\n",
    "\n",
    "#True Negative Calculation\n",
    "TN1=(((cm[1][1])/Total_FN_TN)*100)\n",
    "\n",
    "#Total TP,TN,FP,FN\n",
    "Total=TP1+FP1+FN1+TN1\n",
    "\n",
    "#Accuracy Calculation\n",
    "accuracy=((TP1+TN1)/Total)*100\n",
    "\n",
    "#Error Rate Calculation\n",
    "error_rate=((FP1+FN1)/Total)*100\n",
    "\n",
    "#Precision Calculation\n",
    "precision=TP1/(TP1+FP1)*100\n",
    "\n",
    "#Recall Calculation\n",
    "recall=TP1/(TP1+FN1)*100\n",
    "\n",
    "#F1 Score\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print(\"\\n\\n\\tResult Generation\")\n",
    "print(\"\\t------------------\")\n",
    "print('\\n\\tTrue positive = ', TP1, '%')\n",
    "print('\\n\\tFalse positive = ', FP1, '%')\n",
    "print('\\n\\tFalse negative = ', FN1, '%')\n",
    "print('\\n\\tTrue negative = ',TN1 , '%')\n",
    "print('\\n\\tAccuracy = ',accuracy , '%')\n",
    "print('\\n\\tError Rate = ',error_rate , '%')\n",
    "print('\\n\\tPrecision = ',precision , '%')\n",
    "print('\\n\\tRecall = ',recall , '%')\n",
    "print('\\n\\tF1-Score = ',f1 , '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139138e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
